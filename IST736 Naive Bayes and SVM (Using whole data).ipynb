{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST736 Proj Gloomy Authors SVM and MNB Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using whole data and seperate them 20%test and 80% train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv(r\"C:\\Users\\wangtao\\Desktop\\data.csv\",encoding='latin1')\n",
    "y=train['author'].values\n",
    "X=train['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16575,) (16575,) (4144,) (4144,)\n",
      "That's not love and you know it Mel said.\"\n",
      "RC\n",
      "Terri hon don't look that way.\"\n",
      "RC\n"
     ]
    }
   ],
   "source": [
    "# the whole data seperated into two parts \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "print(X_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RY', 'JJ', 'RC'}\n",
      "[['JJ' 1502]\n",
      " ['RC' 1174]\n",
      " ['RY' 1468]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangtao\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `itemfreq` is deprecated!\n",
      "`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Checking the data and I find some 'NAN's then I finde them and deleted \n",
    "#import numpy as np\n",
    "training_labels = set(y_train)\n",
    "print(training_labels)\n",
    "from scipy.stats import itemfreq\n",
    "test_category_dist = itemfreq(y_test)\n",
    "print(test_category_dist)\n",
    "#training_catagory_dist = np.unique(y_train)\n",
    "#print(training_category_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [chapter, author, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df=pd.DataFrame(train)\n",
    "print(np.any(df.isnull())==True)#打印出至少一个丢失了的数据  另外 简单的表格 可以用 df.isnull() pirnt 出来检查\n",
    "#df.fillna(value=0) 替换掉nan  \n",
    "print((np.where(df.isnull()==True))[0])\n",
    "df.iloc[(np.where(df.isnull()==True))[0]]#打印Nan 所在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=5,stop_words='english')\n",
    "bigram_count_vectoritzer = TfidfVectorizer(encoding='latin-1',binary=False, ngram_range=(1,2),min_df=5,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16575, 3488)\n",
      "[[0 0 0 ... 0 0 0]]\n",
      "3488\n",
      "[('kinda', 1666), ('hotel', 1483), ('partner', 2193), ('open', 2119), ('expression', 1020), ('interested', 1566), ('knock', 1681), ('compartment', 614), ('free', 1189), ('sullen', 3013)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit vocabulary in training documents and transform the training documents into vectors\n",
    "#X_train_vec = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_train_vec = bigram_count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X_train_vec.shape)\n",
    "print(X_train_vec[0].toarray())\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "#print(len(unigram_count_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "#print(list(unigram_count_vectorizer.vocabulary_.items())[:10])\n",
    "print(list(bigram_count_vectorizer.vocabulary_.items())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4144, 3488)\n"
     ]
    }
   ],
   "source": [
    "#Vectorize the test data\n",
    "#X_test_vec = unigram_count_vectorizer.transform(X_test)\n",
    "X_text_vect = bigram_count_vectorizer.tranform(X_test)\n",
    "# print out #examples and #features in the test set\n",
    "print(X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MNB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train MNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-5.305838516606873, 'thing')\n",
      "(-5.202460162153039, 'don')\n",
      "(-5.198005811803658, 'just')\n",
      "(-5.184760585053637, 'right')\n",
      "(-5.176026905084883, 'way')\n",
      "(-5.1545206998639195, 'know')\n",
      "(-5.116937988714876, 'little')\n",
      "(-4.930526446669397, 'like')\n",
      "(-4.903679196633209, 'time')\n",
      "(-3.942073268547011, 'said')\n",
      "\n",
      "(-10.614106214008078, 'abroad')\n",
      "(-10.614106214008078, 'accident')\n",
      "(-10.614106214008078, 'accompanist')\n",
      "(-10.614106214008078, 'account')\n",
      "(-10.614106214008078, 'acts')\n",
      "(-10.614106214008078, 'adventure')\n",
      "(-10.614106214008078, 'aid')\n",
      "(-10.614106214008078, 'alarmed')\n",
      "(-10.614106214008078, 'albert')\n",
      "(-10.614106214008078, 'alittle')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_ranks = sorted(zip(nb_clf.coef_[0], unigram_count_vectorizer.get_feature_names()))\n",
    "JJ_10 = feature_ranks[-10:]\n",
    "for i in range(0, len(JJ_10)):\n",
    "    print(JJ_10[i])\n",
    "print()\n",
    "\n",
    "feature_ranks = sorted(zip(nb_clf.coef_[0], unigram_count_vectorizer.get_feature_names()))\n",
    "not_JJ_10 = feature_ranks[:10]\n",
    "for i in range(0, len(not_JJ_10)):\n",
    "    print(not_JJ_10[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7719594594594594"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.score(X_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1229  155  118]\n",
      " [ 177 1132  159]\n",
      " [ 156  180  838]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         JJ       0.79      0.82      0.80      1502\n",
      "         RY       0.75      0.71      0.73      1174\n",
      "         RC       0.77      0.77      0.77      1468\n",
      "\n",
      "avg / total       0.77      0.77      0.77      4144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = nb_clf.predict(X_test_vec)\n",
    "cm = confusion_matrix(y_test,y_pred,labels=['JJ','RY','RC'])\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['JJ','RY','RC']\n",
    "print(classification_report(y_test,y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LinearSVC Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_clf = LinearSVC(C=1)\n",
    "svm_clf.fit(X_train_vec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.826690215305478, 'lenehan')\n",
      "(1.8521491832420724, 'blouse')\n",
      "(1.8761165307688232, 'chandler')\n",
      "(1.8838378129205338, 'colour')\n",
      "(1.9533384270038006, 'chief')\n",
      "(1.9571143528336608, 'maria')\n",
      "(1.9738715267332492, 'sing')\n",
      "(2.0836284261210185, 'briton')\n",
      "(2.12512981472834, 'gabriel')\n",
      "(2.4683480581091266, 'corley')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#feature_rank = sorted(zip(svm_clf.coef_[0],unigram_count_vectorizer.get_feature_names()))\n",
    "feature_rank = sorted (zip(svm_clf.coef_[0],bigram_count_vectorizer.get_feature_names()))\n",
    "a=feature_rank[-50:]\n",
    "for i in range(0,len(a)):\n",
    "    print(a[i])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8211872586872587"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.score(X_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1257   77  168]\n",
      " [ 100 1157  211]\n",
      " [  77  108  989]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         JJ       0.88      0.84      0.86      1502\n",
      "         RY       0.72      0.84      0.78      1174\n",
      "         RC       0.86      0.79      0.82      1468\n",
      "\n",
      "avg / total       0.83      0.82      0.82      4144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm = confusion_matrix(y_test,y_pred,labels=['JJ','RY','RC'])\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['JJ','RY','RC']\n",
    "print(classification_report(y_test,y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
