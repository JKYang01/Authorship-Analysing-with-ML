{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Get 50 feature words\n",
    "\n",
    "apply linearSVC or extract features that may be the characteristics of different author and then using those features along with the text releted features and train some classifier on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv(r\"C:\\Users\\wangtao\\Desktop\\train_data600.csv\",encoding='latin1')\n",
    "y_train=train['author'].values\n",
    "X_train=train['text'].values\n",
    "test=pd.read_csv(r\"C:\\Users\\wangtao\\Desktop\\test_data600.csv\",encoding='latin1')\n",
    "y_test=test['author'].values\n",
    "X_test=test['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RC', 'RY', 'JJ'}\n",
      "['JJ' 'RC' 'RY']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "training_labels = set(y_train)\n",
    "print(training_labels)\n",
    "#from scipy.stats import itemfreq\n",
    "#test_category_dist = itemfreq(y_test)\n",
    "#print(test_category_dist)\n",
    "training_category_dist = np.unique(y_train)\n",
    "print(training_category_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "## how to add in my own stopword list https://www.kaggle.com/c/msk-redefining-cancer-treatment/discussion/37679\n",
    "from sklearn.feature_extraction import text \n",
    "## Add in stop words using  .union()\n",
    "#stop_words_add = text.ENGLISH_STOP_WORDS.union(['mr','dr','martin','mary','jane','julia','lily',\n",
    "#                                                'maria','jimmy','keran','dillion','mr fixit',\n",
    "#                                               'mrs','mr browne','ll'])\n",
    "## Using my own stop words list\n",
    "stop_words_list = [x.strip() for x in open(r'C:\\Users\\wangtao\\Desktop\\stopwords.txt','r').read().split(',')]\n",
    "\n",
    "bigram_tf = CountVectorizer(min_df=5, stop_words=list(stop_words_list),ngram_range=(1,2),encoding='latin1')\n",
    "#bigram_tfidf = TfidfVectorizer(use_idf=True,min_df=5, stop_words=stop_words_add,ngram_range=(1,2),encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorizer can do \"fit\" and \"transform\"\n",
    "# fit is a process to collect unique tokens into the vocabulary\n",
    "# transform is a process to convert each document to vector based on the vocabulary\n",
    "# These two processes can be done together using fit_transform(), or used individually: fit() or transform()\n",
    "X_train_vec = bigram_tf.fit_transform(X_train)\n",
    "#X_train_tfidfvec = bigram_tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_clf = LinearSVC(C=1)\n",
    "svm_clf.fit(X_train_vec,y_train)\n",
    "#svm_clf.fit(X_train_tfidfvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.009246137073142374, 'kid')\n",
      "(0.009276751221242137, 'fish')\n",
      "(0.009289666115604184, 'house')\n",
      "(0.009375497420700783, 'just')\n",
      "(0.009452868009980301, 'those')\n",
      "(0.009548443288428665, 'put')\n",
      "(0.009552245319160171, 'call')\n",
      "(0.009564533884326218, 'around')\n",
      "(0.00957864487722762, 'goes')\n",
      "(0.009845073486779526, 'woman')\n",
      "(0.010113571129814957, 'they')\n",
      "(0.010220464496758268, 'say')\n",
      "(0.010435947584806574, 'started')\n",
      "(0.010476349170415834, 'days')\n",
      "(0.010636885724064557, 'window')\n",
      "(0.010945684644954273, 'can')\n",
      "(0.011160826858023972, 'bass')\n",
      "(0.011662127746033962, 'guard')\n",
      "(0.011965591288398386, 'newspaper')\n",
      "(0.011966500264698482, 'things')\n",
      "(0.012180406919146996, 'said')\n",
      "(0.01219156165688178, 'went')\n",
      "(0.012226440365205152, 'into')\n",
      "(0.01228881509106607, 'took')\n",
      "(0.012353564493055474, 'laura said')\n",
      "(0.01251753778243383, 'seen')\n",
      "(0.013559955981632232, 'kitchen')\n",
      "(0.013574907291347625, 'go')\n",
      "(0.014233597758035028, 'glass')\n",
      "(0.01429076639612783, 'says')\n",
      "(0.01454113144914347, 'she')\n",
      "(0.014650119140923847, 'he')\n",
      "(0.014684444492826767, 'one')\n",
      "(0.014750339618017388, 'back')\n",
      "(0.015335628493898306, 'looked')\n",
      "(0.01599329704949629, 'table')\n",
      "(0.016201843209099752, 'up')\n",
      "(0.017442274901225782, 'water')\n",
      "(0.017795692737589637, 'my')\n",
      "(0.018379051711170064, 'head')\n",
      "(0.01864601236807615, 'dad')\n",
      "(0.019917613050806707, 'laura')\n",
      "(0.020511191959511742, 'we')\n",
      "(0.02238146534482713, 'but')\n",
      "(0.02340652209710606, 'it')\n",
      "(0.02629355898027502, 'going')\n",
      "(0.026738341060995718, 'on')\n",
      "(0.030830509896560325, 'love')\n",
      "(0.03167805947130617, 'out')\n",
      "(0.04206725296762045, 'dummy')\n"
     ]
    }
   ],
   "source": [
    "feature_rank1= sorted(zip(svm_clf.coef_[1],bigram_tf.get_feature_names()))\n",
    "a1=feature_rank1[-50:]\n",
    "for i in range(0,50):\n",
    "    print(a1[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get 50 feature words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.011890557393890265, 'siz')\n",
      "(0.012029472033609764, 'only')\n",
      "(0.012084983202090586, 'and it')\n",
      "(0.012231005941709033, 'with')\n",
      "(0.012344289396942147, 'lieutenant')\n",
      "(0.012562950566623007, 'to be')\n",
      "(0.012651058173018816, 'home')\n",
      "(0.012809966897619336, 'make')\n",
      "(0.013046631836051276, 'goodnight')\n",
      "(0.013107229832908684, 'well')\n",
      "(0.013112030440465933, 'made')\n",
      "(0.013728042391494229, 'gave')\n",
      "(0.013859966174601931, 'of his')\n",
      "(0.014175951067184781, 'right')\n",
      "(0.014184862953402617, 'she said')\n",
      "(0.014235529115146188, 'look')\n",
      "(0.014284473822670436, 'yew')\n",
      "(0.014318708021017638, 'guess')\n",
      "(0.014322662361528408, 'week')\n",
      "(0.014598489101945392, 'job')\n",
      "(0.014602115445200728, 'smile')\n",
      "(0.014716079870364288, 'lips')\n",
      "(0.014807541833814383, 'wasn')\n",
      "(0.014864536138521414, 'time')\n",
      "(0.015501923091837694, 'half')\n",
      "(0.015593993874242184, 'ralph')\n",
      "(0.015659843719690186, 'don')\n",
      "(0.01586362939340596, 'off')\n",
      "(0.016953345030889303, 'anyway')\n",
      "(0.017175327552721668, 'ya')\n",
      "(0.01749620882990279, 'got')\n",
      "(0.01808896373076504, 'could')\n",
      "(0.018997622807747513, 'price')\n",
      "(0.019058108139107695, 'kind of')\n",
      "(0.019563540888104407, 'now')\n",
      "(0.020114039999367332, 'about')\n",
      "(0.020153547277905796, 'building')\n",
      "(0.021129552760680927, 'kind')\n",
      "(0.02152791967721741, 'tiny')\n",
      "(0.021772367879586543, 'be')\n",
      "(0.021773433159648636, 'he said')\n",
      "(0.021916761892917955, 'you')\n",
      "(0.022337549833271048, 'or')\n",
      "(0.023081731682010966, 'new')\n",
      "(0.024630138954473114, 'thing')\n",
      "(0.02922238406493234, 'around')\n",
      "(0.03151240608391272, 'oh')\n",
      "(0.03217710336271756, 'that')\n",
      "(0.03604059774301138, 'way')\n",
      "(0.04453647532266211, 'all')\n"
     ]
    }
   ],
   "source": [
    "feature_rank = sorted(zip(svm_clf.coef_[0],bigram_tf.get_feature_names()))\n",
    "feature_rank1= sorted(zip(svm_clf.coef_[1],bigram_tf.get_feature_names()))\n",
    "feature_rank2 = sorted(zip(svm_clf.coef_[2],bigram_tf.get_feature_names()))\n",
    "\n",
    "a=feature_rank[-50:]\n",
    "a1=feature_rank1[-50:]\n",
    "a2=feature_rank2[-50:]\n",
    "l=[]\n",
    "l1=[]\n",
    "l2=[]\n",
    "for i in range(0,50):\n",
    "    print(a2[i])\n",
    "    l.append(a[i])\n",
    "    l1.append(a1[i])\n",
    "    l2.append(a2[i])\n",
    "f_data = {'James Joyce':l,'Raymond Carver':l1,'Richard Yates':l2}\n",
    "\n",
    "df_f_data=pd.DataFrame(f_data)\n",
    "\n",
    "with open (r'C:\\Users\\wangtao\\Desktop\\featurewords.csv','w')as f:\n",
    "    df_f_data.to_csv(f)\n",
    "\n",
    "\n",
    "# This is a function I found from stackexchange, and adapted a little bit\n",
    "# The purpose is to print the top and bottom features nicely\n",
    "# https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers\n",
    "\n",
    "# You can find many useful scripts from stackexchange or GitHub\n",
    "# Most tasks are not so unique, so someone in this world might have done something similar and shared their code\n",
    "\n",
    "#def show_most_and_least_informative_features(vectorizer, clf, class_idx=0, n=10):\n",
    "#    feature_names = vectorizer.get_feature_names()\n",
    "#    coefs_with_fns = sorted(zip(clf.feature_log_prob_[class_idx], feature_names))\n",
    "#    top = zip(coefs_with_fns[:n], coefs_with_fns[-n:])\n",
    "#    for (coef_2, fn_2) in top:\n",
    "#        print(\"\\t%.4f\\t%-15s\" %  coef_2, fn_2)\n",
    "    \n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about the scores that indicate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = bigram_tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975609756097561"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.score(X_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if making one sentece per dobument the score is 0.5486404833836858\n",
    "and score is much higher when change the size of each dobuments and the number of documents!!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Seems not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0]\n",
      " [ 0 15  1]\n",
      " [ 0  0 10]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         JJ       1.00      1.00      1.00        15\n",
      "         RY       0.91      1.00      0.95        10\n",
      "         RC       1.00      0.94      0.97        16\n",
      "\n",
      "avg / total       0.98      0.98      0.98        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = svm_clf.predict(X_test_vec)\n",
    "cm = confusion_matrix(y_test,y_pred,labels=['JJ','RY','RC'])\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['JJ','RY','RC']\n",
    "print(classification_report(y_test,y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun with a stranger can you?\" She gave them a homely shy smile. And that's just the way it is with words. When she said something like that it was more embarrassing than anything else but it did leave the children with a certain vague sense of responsibility toward her and often prompted them into a loyal reticence when children from other classes demanded to know how bad she really was. Well not too bad they would say uncomfortably and try to change the subject. John Gerhardt and Howard White usually walked home from school together and often as not though they tried to avoid it they were joined by two of the children from Mrs.Cleary's class who lived on their street Freddy Taylor and his twin sister Grace. John and Howard usually got about as far as the end of the playground before the twins came running after them out of the crowd. Hey wait up! Freddy would call. Wait up! And in a moment the twins would fall into step beside them chattering swinging their identical plaid canvas schoolbags. Guess what we're gonna do next week Freddy said in his chirping voice one afternoon. Our whole class I mean. Guess.\" Come on guess.\" John Gerhardt had already made it plain to the twins once in so many words that he didn't like walking home with a girl and now he very nearly said something to the effect that one girl was bad enough but two were more than he could take. Instead he aimed a knowing glance at Howard White and they both walked on in silence determined not to answer Freddy's insistent \"Guess. But Freddy didn't wait long for an answer. We're gonna take a field trip he said \"for our class in Transportation. We're gonna go to Harmon. You know what Harmon is? Sure Howard White said. A town. No but I mean you know what they do there? What they do is that's where they change all the trains coming into New York from steam locomotives to electric power. Mrs.Cleary says we're gonna watch 'em changing the locomotives and everything.\" We're gonna spend practically the whole day Grace said. So what's so great about that? Howard White asked. I can go there any day if I feel like it on my bike. This was an exaggeration he wasn't allowed out of a two-block radius on his bike but it sounded good especially when he added \"I don't need any Mrs.Cleary to take me\" with a mincing sissy emphasis on the \"Cleary. On a school day? Grace inquired. Can you go on a school day? Lamely Howard murmured \"Sure if I feel like it\" but it was a clear point for the twins. Mrs.Cleary says we're gonna take a lotta field trips Freddy said. Later on we're gonna go to the Museum of Natural History in New York and a whole lotta other places. Too bad you're not in Mrs.Cleary's class.\" Doesn't bother me any John Gerhardt said. Then he came up with a direct quotation from his father that seemed appropriate: \"Anyway I don't go to school to fool around. I go to school to work. Come on Howard. A day or two later it turned out that both classes were scheduled to take the field trip together; Miss Snell had just neglected to tell her pupils about it. When she did tell them it was in one of her nice moods. I think the trip will be especially valuable she said \"because it will be instructive and at the same time it will be\n",
      "27\n",
      "errors: 1\n"
     ]
    }
   ],
   "source": [
    "# print out specific type of error for further analysis\n",
    "\n",
    "# print out the very positive examples that are mistakenly predicted as negative\n",
    "# according to the confusion matrix, there should be 53 such examples\n",
    "# note if you use a different vectorizer option, your result might be different\n",
    "\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i]=='RY' and y_pred[i]=='RC'):\n",
    "        print(X_test[i])\n",
    "        print(i)\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_tfidfvec = bigram_tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8663490471414242"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#svm_clf.score(X_test_tfidfvec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2573  107  254]\n",
      " [ 113 2186  287]\n",
      " [ 153  152 2151]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         JJ       0.91      0.88      0.89      2934\n",
      "         RY       0.80      0.88      0.84      2456\n",
      "         RC       0.89      0.85      0.87      2586\n",
      "\n",
      "avg / total       0.87      0.87      0.87      7976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import confusion_matrix\n",
    "#y_pred = svm_clf.predict(X_test_tfidfvec)\n",
    "#cm = confusion_matrix(y_test,y_pred,labels=['JJ','RY','RC'])\n",
    "#print(cm)\n",
    "\n",
    "#from sklearn.metrics import classification_report\n",
    "#target_names = ['JJ','RY','RC']\n",
    "#print(classification_report(y_test,y_pred,target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
